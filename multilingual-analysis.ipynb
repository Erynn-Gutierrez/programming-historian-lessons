{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb6c6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.  Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».  — Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.  Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.  — Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n"
     ]
    }
   ],
   "source": [
    "war_and_peace = \"\"\"\n",
    "— Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\n",
    "\n",
    "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:\n",
    "\n",
    "«Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».\n",
    "\n",
    "— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
    "\n",
    "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
    "\n",
    "— Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n",
    "\"\"\"\n",
    "\n",
    "cleaned_war_and_peace = war_and_peace.replace(\"\\n\", \" \").strip()\n",
    "print(cleaned_war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bacd89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf0e17",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51f08567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk_sent_tokenized = sent_tokenize(cleaned_war_and_peace)\n",
    "# if you were going to specify a language, the following syntax would be used: nltk_sent_tokenized = sent_tokenize(war_and_peace, language=\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ad2c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Eh bien, mon prince.\n",
      "Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte.\n",
      "Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites.\n",
      "Ну, здравствуйте, здравствуйте.\n",
      "Je vois que je vous fais peur, садитесь и рассказывайте.\n",
      "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими).\n",
      "В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures.\n",
      "Annette Scherer».\n",
      "— Dieu, quelle virulente sortie!\n",
      "— отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
      "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку.\n",
      "Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
      "— Avant tout dites moi, comment vous allez, chère amie?\n",
      "Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n"
     ]
    }
   ],
   "source": [
    "# printing each sentence in our list\n",
    "for sent in nltk_sent_tokenized:\n",
    "  print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36c13af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.\n"
     ]
    }
   ],
   "source": [
    "# printing the Russian sentence at index 5 in our list of sentences\n",
    "rus_sent = nltk_sent_tokenized[5]\n",
    "print('Russian: ' + rus_sent)\n",
    "\n",
    "# printing the French sentence at index 2\n",
    "fre_sent = nltk_sent_tokenized[13]\n",
    "print('French: ' + fre_sent)\n",
    "\n",
    "# printing the sentence in both French and Russian at index 4\n",
    "multi_sent = nltk_sent_tokenized[4]\n",
    "print('Multilang: ' + multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798ac78",
   "metadata": {},
   "source": [
    "## Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d6a9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[— Eh bien, mon prince., Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte., Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites., Ну, здравствуйте, здравствуйте., Je vois que je vous fais peur, садитесь и рассказывайте.  , Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер., Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими)., В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  , «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures., Annette Scherer».  , — Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.  , Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку., Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.  , — Avant tout dites moi, comment vous allez, chère amie?, Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.]\n"
     ]
    }
   ],
   "source": [
    "# downloading our multilingual sentence tokenizer\n",
    "# python -m spacy download xx_sent_ud_sm\n",
    "\n",
    "# loading the multilingual sentence tokenizer we just downloaded\n",
    "nlp = spacy.load(\"xx_sent_ud_sm\")\n",
    "# applying the spaCy model to our text variable\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# assigning the tokenized sentences to a list so it's easier for us to manipulate them later\n",
    "spacy_sentences = list(doc.sents)\n",
    "\n",
    "# printing the sentences to our console\n",
    "print(spacy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb1b0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.  \n"
     ]
    }
   ],
   "source": [
    "# concatenating the Russian sentence and its language label\n",
    "spacy_rus_sent = str(spacy_sentences[5])\n",
    "print('Russian: ' + spacy_rus_sent)\n",
    "\n",
    "# concatenating the French sentence and its language label\n",
    "spacy_fre_sent = str(spacy_sentences[13])\n",
    "print('French: ' + spacy_fre_sent)\n",
    "\n",
    "# concatenating the French and Russian sentence and its label\n",
    "spacy_multi_sent = str(spacy_sentences[4])\n",
    "print('Multilang: ' + spacy_multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4973e24",
   "metadata": {},
   "source": [
    "## Tokenization with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9586d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 14:57:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 123MB/s]                     \n",
      "2025-04-17 14:57:11 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-multilingual/resolve/v1.10.0/models/langid/ud.pt: 100%|██████████| 9.07M/9.07M [00:00<00:00, 152MB/s]\n",
      "2025-04-17 14:57:12 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-04-17 14:57:12 INFO: Using device: cpu\n",
      "2025-04-17 14:57:12 INFO: Loading: langid\n",
      "2025-04-17 14:57:12 INFO: Done loading processors!\n",
      "2025-04-17 14:57:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 182MB/s]                     \n",
      "2025-04-17 14:57:12 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/tokenize/syntagrus.pt: 100%|██████████| 641k/641k [00:00<00:00, 34.7MB/s]\n",
      "2025-04-17 14:57:12 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2025-04-17 14:57:12 INFO: Using device: cpu\n",
      "2025-04-17 14:57:12 INFO: Loading: tokenize\n",
      "2025-04-17 14:57:14 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['— Eh bien, mon prince.', 'Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte.', 'Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites.', 'Ну, здравствуйте, здравствуйте.', 'Je vois que je vous fais peur, садитесь и рассказывайте.', 'Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.', 'Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими).', 'В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures.', 'Annette Scherer».', '— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.', 'Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку.', 'Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.', '— Avant tout dites moi, comment vous allez, chère amie?', 'Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.']\n"
     ]
    }
   ],
   "source": [
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# setting up our tokenizer pipeline\n",
    "nlp = MultilingualPipeline(processors='tokenize')\n",
    "\n",
    "# applying the pipeline to our text\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# printing all sentences to see how they tokenized\n",
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "433a923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list to append our sentences to\n",
    "stanza_sentences = []\n",
    "\n",
    "# iterating through the sentence tokens created by the tokenizer pipeline and appending to the list\n",
    "for sentence in doc.sentences:\n",
    "  stanza_sentences.append(sentence.text)\n",
    "\n",
    "# printing our sentence that is only in Russian\n",
    "stanza_rus_sent = str(stanza_sentences[5])\n",
    "print('Russian: ' + stanza_rus_sent)\n",
    "\n",
    "# printing our sentence that is only in French\n",
    "stanza_fre_sent = str(stanza_sentences[12])\n",
    "print('French: ' + stanza_fre_sent)\n",
    "\n",
    "# printing our sentence in both French and Russian\n",
    "stanza_multi_sent = str(stanza_sentences[4])\n",
    "print('Multilang: ' + stanza_multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37dbed",
   "metadata": {},
   "source": [
    "## Automatically Detecting Different Languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcb5e94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package crubadan to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/crubadan.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian estimate: rus\n",
      "French estimate: fra\n",
      "Multilingual estimate: rus\n"
     ]
    }
   ],
   "source": [
    "# downloading an NLTK corpus reader required by the TextCat module\n",
    "nltk.download('crubadan')\n",
    "\n",
    "# loading the TextCat package and applying it to each of our sentences\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "rus_estimate = tcat.guess_language(rus_sent)\n",
    "fre_estimate = tcat.guess_language(fre_sent)\n",
    "multi_estimate = tcat.guess_language(multi_sent)\n",
    "\n",
    "# printing the results\n",
    "print('Russian estimate: ' + rus_estimate)\n",
    "print('French estimate: ' + fre_estimate)\n",
    "print('Multilingual estimate: ' + multi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3bb8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy_langdetect\n",
      "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pytest (from spacy_langdetect)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting langdetect==1.0.7 (from spacy_langdetect)\n",
      "  Downloading langdetect-1.0.7.zip (998 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from langdetect==1.0.7->spacy_langdetect) (1.17.0)\n",
      "Collecting iniconfig (from pytest->spacy_langdetect)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from pytest->spacy_langdetect) (24.2)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->spacy_langdetect)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993519 sha256=f6d2c93aa752d4f467235965487b71428ce0eaf278b2b75d4ef0bc084f3a4eda\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/bc/5d/e7/dfa384408d6ca60501788f512a6d4f6e73cc60f3ad7c75faee\n",
      "Successfully built langdetect\n",
      "Installing collected packages: pluggy, langdetect, iniconfig, pytest, spacy_langdetect\n",
      "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed iniconfig-2.1.0 langdetect-1.0.7 pluggy-1.5.0 pytest-8.3.5 spacy_langdetect-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42ffd7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 14:59:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 95.2MB/s]                    \n",
      "2025-04-17 14:59:15 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 14:59:15 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-04-17 14:59:15 INFO: Using device: cpu\n",
      "2025-04-17 14:59:15 INFO: Loading: langid\n",
      "2025-04-17 14:59:15 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\tru\n",
      "— Avant tout dites moi, comment vous allez, chère amie?\tfr\n",
      "Je vois que je vous fais peur, садитесь и рассказывайте.\tfr\n"
     ]
    }
   ],
   "source": [
    "# importing our models required for language detection\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "\n",
    "# setting up our pipeline\n",
    "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\")\n",
    "\n",
    "# specifying which sentences to run the detection on, then running the detection code\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "docs = [Document([], text=text) for text in docs]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the text of each sentence alongside the language estimates\n",
    "print(\"\\n\".join(f\"{doc.text}\\t{doc.lang}\" for doc in docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1bb77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokenized_sent = wordpunct_tokenize(multi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9249457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the regex package so we can use a regular expression\n",
    "import regex\n",
    "# importing the string package to detect punctuation\n",
    "from string import punctuation\n",
    "\n",
    "# setting empty lists we will later populate with our words\n",
    "cyrillic_words = []\n",
    "latin_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "547d6e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['садитесь', 'и', 'рассказывайте']\n",
      "['Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur']\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_sent:\n",
    "  if word in punctuation:\n",
    "    continue\n",
    "  else:\n",
    "    if regex.search(r'\\p{IsCyrillic}', word):\n",
    "      cyrillic_words.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe2755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyrillic estimate: rus\n",
      "Latin estimate: fra\n"
     ]
    }
   ],
   "source": [
    "# joining the lists into a string, with each word separated by a space (' ')\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# now we use TextCat again to detect their languages\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "multi_estimate_1 = tcat.guess_language(cyrillic_only_list)\n",
    "multi_estimate_2 = tcat.guess_language(latin_only_list)\n",
    "\n",
    "# printing our estimates\n",
    "print('Cyrillic estimate: ' + multi_estimate_1)\n",
    "print('Latin estimate: ' + multi_estimate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f8630",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88ab2e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так ADV\n",
      "говорила VERB\n",
      "в ADP\n",
      "июле NOUN\n",
      "1805 ADJ\n",
      "года NOUN\n",
      "известная ADJ\n",
      "Анна PROPN\n",
      "Павловна PROPN\n",
      "Шерер PROPN\n",
      ", PUNCT\n",
      "фрейлина NOUN\n",
      "и CCONJ\n",
      "приближенная ADJ\n",
      "императрицы NOUN\n",
      "Марии PROPN\n",
      "Феодоровны PROPN\n",
      ", PUNCT\n",
      "встречая VERB\n",
      "важного ADJ\n",
      "и CCONJ\n",
      "чиновного ADJ\n",
      "князя NOUN\n",
      "Василия PROPN\n",
      ", PUNCT\n",
      "первого ADJ\n",
      "приехавшего VERB\n",
      "на ADP\n",
      "ее DET\n",
      "вечер NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# downloading our Russian model from spaCy\n",
    "# python -m spacy download ru_core_news_sm\n",
    "\n",
    "\n",
    "# loading the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_rus_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c86dcd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— PUNCT\n",
      "Avant ADP\n",
      "tout PRON\n",
      "dites VERB\n",
      "moi PRON\n",
      ", PUNCT\n",
      "comment ADV\n",
      "vous PRON\n",
      "allez VERB\n",
      ", PUNCT\n",
      "chère ADJ\n",
      "amie NOUN\n",
      "? PUNCT\n"
     ]
    }
   ],
   "source": [
    "# downloading our French model from spaCy\n",
    "# python -m spacy download fr_core_news_sm\n",
    "\n",
    "\n",
    "# loading the corpus\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_fre_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa977c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['садитесь', 'и', 'рассказывайте']\n",
      "['Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur', 'Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur']\n"
     ]
    }
   ],
   "source": [
    "# creating our blank lists to append to later\n",
    "cyrillic_words_punct = []\n",
    "latin_words_punct = []\n",
    "\n",
    "# initializing a blank string to keep track of the last list we appended to\n",
    "last_appended_list = ''\n",
    "\n",
    "# iterating through our words and appending based on whether a Cyrillic character was detected\n",
    "for word in tokenized_sent:\n",
    "  if regex.search(r'\\p{IsCyrillic}', word):\n",
    "    cyrillic_words_punct.append(word)\n",
    "    # updating our string to track the list we appended a word to\n",
    "    last_appended_list = 'cyr'\n",
    "  else:\n",
    "    # handling punctuation by appending it to our most recently used list\n",
    "    if word in punctuation:\n",
    "        if last_appended_list == 'cyr':\n",
    "            cyrillic_words_punct.append(word)\n",
    "        elif last_appended_list == 'lat':\n",
    "            latin_words_punct.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "        last_appended_list = 'lat'\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73ac6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь и рассказывайте\n",
      "Je vois que je vous fais peur Je vois que je vous fais peur\n"
     ]
    }
   ],
   "source": [
    "# joining the lists to strings\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# using our regular expression to remove extra whitespace before the punctuation marks\n",
    "cyr_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', cyrillic_only_list)\n",
    "lat_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', latin_only_list)\n",
    "\n",
    "# checking the results of the regular expression above\n",
    "print(cyr_no_extra_space)\n",
    "print(lat_no_extra_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63802995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь VERB\n",
      "и CCONJ\n",
      "рассказывайте VERB\n",
      "Je PRON\n",
      "vois VERB\n",
      "que SCONJ\n",
      "je PRON\n",
      "vous PRON\n",
      "fais VERB\n",
      "peur NOUN\n",
      "Je PRON\n",
      "vois VERB\n",
      "que SCONJ\n",
      "je PRON\n",
      "vous PRON\n",
      "fais VERB\n",
      "peur NOUN\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# and doing the same with our French sentence\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b94ab69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:01:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 192MB/s]                     \n",
      "2025-04-17 15:01:47 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/pos/syntagrus_charlm.pt: 100%|██████████| 38.2M/38.2M [00:00<00:00, 173MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/pretrain/conll17.pt: 100%|██████████| 109M/109M [00:00<00:00, 194MB/s]  \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/forward_charlm/newswiki.pt: 100%|██████████| 20.0M/20.0M [00:00<00:00, 329MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/backward_charlm/newswiki.pt: 100%|██████████| 20.0M/20.0M [00:00<00:00, 317MB/s]\n",
      "2025-04-17 15:01:48 INFO: Loading these models for language: ru (Russian):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | syntagrus        |\n",
      "| pos       | syntagrus_charlm |\n",
      "================================\n",
      "\n",
      "2025-04-17 15:01:48 INFO: Using device: cpu\n",
      "2025-04-17 15:01:48 INFO: Loading: tokenize\n",
      "2025-04-17 15:01:48 INFO: Loading: pos\n",
      "2025-04-17 15:01:50 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Так\tupos: ADV\n",
      "word: говорила\tupos: VERB\n",
      "word: в\tupos: ADP\n",
      "word: июле\tupos: NOUN\n",
      "word: 1805\tupos: ADJ\n",
      "word: года\tupos: NOUN\n",
      "word: известная\tupos: ADJ\n",
      "word: Анна\tupos: PROPN\n",
      "word: Павловна\tupos: PROPN\n",
      "word: Шерер\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: фрейлина\tupos: NOUN\n",
      "word: и\tupos: CCONJ\n",
      "word: приближенная\tupos: ADJ\n",
      "word: императрицы\tupos: NOUN\n",
      "word: Марии\tupos: PROPN\n",
      "word: Феодоровны\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: встречая\tupos: VERB\n",
      "word: важного\tupos: ADJ\n",
      "word: и\tupos: CCONJ\n",
      "word: чиновного\tupos: ADJ\n",
      "word: князя\tupos: NOUN\n",
      "word: Василия\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: первого\tupos: ADJ\n",
      "word: приехавшего\tupos: VERB\n",
      "word: на\tupos: ADP\n",
      "word: ее\tupos: DET\n",
      "word: вечер\tupos: NOUN\n",
      "word: .\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as Russian ('ru')\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')\n",
    "doc = nlp(stanza_rus_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9597e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:01:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 107MB/s]                     \n",
      "2025-04-17 15:01:56 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/tokenize/combined.pt: 100%|██████████| 665k/665k [00:00<00:00, 83.6MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/mwt/combined.pt: 100%|██████████| 545k/545k [00:00<00:00, 86.5MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/pos/combined_charlm.pt: 100%|██████████| 34.7M/34.7M [00:00<00:00, 277MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/pretrain/conll17.pt: 100%|██████████| 107M/107M [00:00<00:00, 226MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/forward_charlm/newswiki.pt: 100%|██████████| 20.1M/20.1M [00:00<00:00, 185MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/backward_charlm/newswiki.pt: 100%|██████████| 20.1M/20.1M [00:00<00:00, 314MB/s]\n",
      "2025-04-17 15:01:58 INFO: Loading these models for language: fr (French):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-04-17 15:01:58 INFO: Using device: cpu\n",
      "2025-04-17 15:01:58 INFO: Loading: tokenize\n",
      "2025-04-17 15:01:58 INFO: Loading: mwt\n",
      "2025-04-17 15:01:58 INFO: Loading: pos\n",
      "2025-04-17 15:02:00 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: —\tupos: PUNCT\n",
      "word: Avant\tupos: ADP\n",
      "word: tout\tupos: PRON\n",
      "word: dites\tupos: VERB\n",
      "word: moi\tupos: PRON\n",
      "word: ,\tupos: PUNCT\n",
      "word: comment\tupos: ADV\n",
      "word: vous\tupos: PRON\n",
      "word: allez\tupos: VERB\n",
      "word: ,\tupos: PUNCT\n",
      "word: chère\tupos: ADJ\n",
      "word: amie\tupos: NOUN\n",
      "word: ?\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as French ('fr')\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos')\n",
    "doc = nlp(stanza_fre_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c65d8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:02:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 67.7MB/s]                    \n",
      "2025-04-17 15:02:05 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-04-17 15:02:05 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-04-17 15:02:05 INFO: Using device: cpu\n",
      "2025-04-17 15:02:05 INFO: Loading: langid\n",
      "2025-04-17 15:02:05 INFO: Done loading processors!\n",
      "2025-04-17 15:02:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 124MB/s]                     \n",
      "2025-04-17 15:02:05 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-04-17 15:02:06 INFO: Loading these models for language: ru (Russian):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | syntagrus        |\n",
      "| pos       | syntagrus_charlm |\n",
      "================================\n",
      "\n",
      "2025-04-17 15:02:06 INFO: Using device: cpu\n",
      "2025-04-17 15:02:06 INFO: Loading: tokenize\n",
      "2025-04-17 15:02:06 INFO: Loading: pos\n",
      "2025-04-17 15:02:08 INFO: Done loading processors!\n",
      "2025-04-17 15:02:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 142MB/s]                     \n",
      "2025-04-17 15:02:08 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-04-17 15:02:08 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-17 15:02:08 INFO: Loading these models for language: fr (French):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-04-17 15:02:08 INFO: Using device: cpu\n",
      "2025-04-17 15:02:08 INFO: Loading: tokenize\n",
      "2025-04-17 15:02:08 INFO: Loading: mwt\n",
      "2025-04-17 15:02:08 INFO: Loading: pos\n",
      "2025-04-17 15:02:10 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: —\tupos: PUNCT\n",
      "word: Avant\tupos: ADP\n",
      "word: tout\tupos: PRON\n",
      "word: dites\tupos: VERB\n",
      "word: moi\tupos: PRON\n",
      "word: ,\tupos: PUNCT\n",
      "word: comment\tupos: ADV\n",
      "word: vous\tupos: PRON\n",
      "word: allez\tupos: VERB\n",
      "word: ,\tupos: PUNCT\n",
      "word: chère\tupos: ADJ\n",
      "word: amie\tupos: NOUN\n",
      "word: ?\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# imports so we can use Stanza's MultilingualPipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# running the multilingual pipeline on our French, Russian, and multilingual sentences simultaneously\n",
    "nlp = MultilingualPipeline(processors='tokenize,pos')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the results\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcf812",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9069d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь садиться\n",
      "и и\n",
      "рассказывайте рассказывать\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ff8b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je je\n",
      "vois voir\n",
      "que que\n",
      "je je\n",
      "vous vous\n",
      "fais faire\n",
      "peur peur\n",
      "Je je\n",
      "vois voir\n",
      "que que\n",
      "je je\n",
      "vous vous\n",
      "fais faire\n",
      "peur peur\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17527eb8",
   "metadata": {},
   "source": [
    "## Lemmatization with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69b74b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:02:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 171MB/s]                     \n",
      "2025-04-17 15:02:44 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-04-17 15:02:44 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-04-17 15:02:44 INFO: Using device: cpu\n",
      "2025-04-17 15:02:44 INFO: Loading: langid\n",
      "2025-04-17 15:02:44 INFO: Done loading processors!\n",
      "2025-04-17 15:02:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 220MB/s]                     \n",
      "2025-04-17 15:02:44 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/lemma/syntagrus_nocharlm.pt: 100%|██████████| 13.5M/13.5M [00:00<00:00, 292MB/s]\n",
      "2025-04-17 15:02:44 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-04-17 15:02:44 INFO: Using device: cpu\n",
      "2025-04-17 15:02:44 INFO: Loading: tokenize\n",
      "2025-04-17 15:02:44 INFO: Loading: lemma\n",
      "2025-04-17 15:02:47 INFO: Done loading processors!\n",
      "2025-04-17 15:02:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 135MB/s]                     \n",
      "2025-04-17 15:02:47 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-04-17 15:02:47 WARNING: Language fr package default expects mwt, which has been added\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100%|██████████| 5.41M/5.41M [00:00<00:00, 227MB/s]\n",
      "2025-04-17 15:02:47 INFO: Loading these models for language: fr (French):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-04-17 15:02:47 INFO: Using device: cpu\n",
      "2025-04-17 15:02:47 INFO: Loading: tokenize\n",
      "2025-04-17 15:02:47 INFO: Loading: mwt\n",
      "2025-04-17 15:02:47 INFO: Loading: lemma\n",
      "2025-04-17 15:02:48 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['так', 'говорить', 'в', 'июль', '1805', 'год', 'известный', 'Анна', 'Павловна', 'Шерер', ',', 'фрейлин', 'и', 'приближенный', 'императрица', 'Мария', 'феодоровный', ',', 'встречать', 'важный', 'и', 'чиновный', 'князь', 'Василий', ',', 'первый', 'приехать', 'на', 'она', 'вечер', '.']\n",
      "['—', 'avant', 'tout', 'dire', 'moi', ',', 'comment', 'vous', 'aller', ',', 'cher', 'amie', '?']\n",
      "['moi', 'voir', 'que', 'moi', 'vous', 'faire', 'peur', ',', 'садитесь', 'и', 'рассказывайте', '.']\n"
     ]
    }
   ],
   "source": [
    "# imports so we can run the multilingual pipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# adding the 'lemma' processor to the pipeline and running it on our sentences\n",
    "nlp = MultilingualPipeline(processors='tokenize,lemma')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlped_docs = nlp(docs)\n",
    "\n",
    "# iterating through each sentence's words and printing the lemmas\n",
    "for doc in nlped_docs:\n",
    "  lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]\n",
    "  print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
